{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import librosa\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit, KFold, GroupKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dropout, LSTM\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import opensmile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Current directory:\", os.getcwd())\n",
    "#print(\"Directory contents:\", os.listdir('.'))\n",
    "\n",
    "from src import lstm, LSTM2\n",
    "from src.data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(y):\n",
    "    enc = OneHotEncoder(sparse_output=False)  \n",
    "    return enc.fit_transform(y.reshape(-1, 1)) \n",
    "\n",
    "\n",
    "def normalization(X):\n",
    "    scaler = StandardScaler()\n",
    "    X_shape = X.shape\n",
    "    if len(X_shape) == 3:\n",
    "        X_reshaped = X.reshape(-1, X.shape[-1])\n",
    "        X_scaled = scaler.fit_transform(X_reshaped)\n",
    "        X = X_scaled.reshape(X_shape[0], X_shape[1], X_shape[2])\n",
    "        return X\n",
    "    else:\n",
    "        print(\"Debería ser dimenison 3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_unprocessed_path = r'data/Audio_Speech_Actors_01-24/*/*.wav'\n",
    "song_unprocessed_path = r'data/Audio_Song_Actors_01-24/*/*.wav'\n",
    "\n",
    "speech_dataset_path = \"data/speech_dataset.npy\"\n",
    "song_dataset_path = \"data/song_dataset.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class Dataloader():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        # super(AudioDataloader, self).__init__(*args, **kwargs)\n",
    "        # self.collate_fn = collate_fn\n",
    "\n",
    "    def segment_audio(self, audio, num_parts):\n",
    "        segment_length = len(audio) // num_parts\n",
    "        segments = [audio[i * segment_length:(i + 1) * segment_length] for i in range(num_parts)]\n",
    "        \n",
    "        # Si el audio no se divide exactamente en partes iguales, añade el residuo al último segmento\n",
    "        if len(audio) % num_parts != 0:\n",
    "            segments[-1] = np.concatenate([segments[-1], audio[num_parts * segment_length:]])\n",
    "        \n",
    "        return segments\n",
    "\n",
    "\n",
    "    def process_dataset(self, data_path, save_path, n_segments = 1):\n",
    "        files = glob(data_path)\n",
    "\n",
    "        smile = opensmile.Smile(\n",
    "            feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
    "            feature_level=opensmile.FeatureLevel.Functionals,\n",
    "        )\n",
    "        extracted_features = smile.process_files(files)\n",
    "\n",
    "        x_ = extracted_features.values\n",
    "        y_ = np.array([int(os.path.basename(path).split('-')[2]) for path in files])\n",
    "        y_reshaped = y_[:, np.newaxis] \n",
    "        dataset = np.concatenate((x_, y_reshaped), axis=1)\n",
    "        \n",
    "        actors = np.array([int(os.path.dirname(path)[-2:]) for path in files])\n",
    "        np.save(save_path, dataset)\n",
    "        actors_save_path = f\"data/{os.path.basename(save_path).split('_')[0]}_actors.npy\"\n",
    "        np.save(actors_save_path, actors)\n",
    "\n",
    "        #np.savez(save_path, array1=dataset, array2=actors)\n",
    "        return np.load(save_path), np.load(actors_save_path)\n",
    "    \n",
    "    def get_dataset(self, dataset_path_list):\n",
    "        dataset = np.load(dataset_path_list[0])\n",
    "        actors_path_list = f\"{os.path.dirname(dataset_path_list[0])}/{os.path.basename(dataset_path_list[0]).split('_')[0]}_actors{os.path.splitext(dataset_path_list[0])[1]}\"\n",
    "        actors = np.load(actors_path_list)\n",
    "        for i in range(1, len(dataset_path_list)):\n",
    "            dataset = np.concatenate((dataset, np.load(dataset_path_list[i])))\n",
    "            actors_path_list = f\"{os.path.dirname(dataset_path_list[i])}/{os.path.basename(dataset_path_list[i]).split('_')[0]}_actors{os.path.splitext(dataset_path_list[i])[1]}\"\n",
    "            actors = np.concatenate((actors, np.load(actors_path_list)))\n",
    "        x = dataset[:, :-1]\n",
    "        y = dataset[:,-1]\n",
    "\n",
    "        \n",
    "        return x, y, actors\n",
    "\n",
    "    def split_dataset(self, x, y, actors = [], n_splits =1):\n",
    "        if len(actors) >0:\n",
    "            gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "            train_idx, test_idx = next(gss.split(x, y, actors))\n",
    "            X_train, X_test = x[train_idx], x[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            actors_train = actors[train_idx]\n",
    "            actors_test = actors[test_idx]\n",
    "       \n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(x, y, shuffle=42)\n",
    "            actors_train = None\n",
    "            actors_test = None\n",
    "        return X_train, X_test, y_train, y_test, actors_train, actors_test\n",
    "\n",
    "DL = Dataloader()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def calculate_energy(audio, frame_size, hop_length):\n",
    "    energy = np.array([np.sum(np.abs(audio[i:i+frame_size]**2)) for i in range(0, len(audio), hop_length)])\n",
    "    return energy\n",
    "\n",
    "def trim_silence(audio, frame_size=1024, hop_length=512, energy_threshold=0.01):\n",
    "    energy = calculate_energy(audio, frame_size, hop_length)\n",
    "    frames = np.nonzero(energy > energy_threshold)[0]\n",
    "    \n",
    "    if len(frames) > 0:\n",
    "        start = max(0, frames[0] * hop_length)\n",
    "        end = min(len(audio), frames[-1] * hop_length + frame_size)\n",
    "        return audio[start:end]\n",
    "    else:\n",
    "        return audio  # No recortar si no se encuentran frames con energía suficiente\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tendria qeu hacer el split de train y test antes para que siempre sea el mismo test\n",
    "\"\"\"\n",
    "class AudioProcessor:\n",
    "    def segment_audio(self, audio, num_parts, frame_size=1024, hop_length=512, energy_threshold=0.01):\n",
    "        # Recortar los silencios basados en la energía\n",
    "        #print(\"\")\n",
    "        #print(\"audio largo\", len(audio))\n",
    "        trimmed_audio = trim_silence(audio, frame_size, hop_length, energy_threshold)\n",
    "        #print(\"trimmed largo\", len(trimmed_audio))\n",
    "        print(\"Largo trimmed\", len(trimmed_audio))\n",
    "        if len(trimmed_audio) < 10000:\n",
    "            plt.plot(audio)\n",
    "            plt.show()\n",
    "            print(\"Largo trimmed\", len(trimmed_audio))\n",
    "            plt.plot(trimmed_audio)\n",
    "            plt.show()\n",
    "\n",
    "        # Segmentar el audio recortado\n",
    "        segment_length = len(trimmed_audio) // num_parts\n",
    "        segments = []\n",
    "\n",
    "        for i in range(num_parts):\n",
    "            start = i * segment_length\n",
    "            end = (i + 1) * segment_length\n",
    "            segment = trimmed_audio[start:end]\n",
    "            \n",
    "            # Si el segmento es demasiado corto, rellenar con ceros\n",
    "            if len(segment) < segment_length:\n",
    "                segment = np.pad(segment, (0, segment_length - len(segment)), 'constant')\n",
    "            \n",
    "            segments.append(segment)\n",
    "        \n",
    "        return segments\n",
    "\n",
    "\n",
    "\n",
    "    def segment_audio2(self, audio, num_parts, top_db=30):\n",
    "        # Recortar los silencios\n",
    "        #print(\"\")\n",
    "        #print(\"audio largo\", len(audio))\n",
    "        trimmed_audio, _ = librosa.effects.trim(audio, top_db=top_db)\n",
    "\n",
    "        #print(\"trimmed largo\", len(trimmed_audio))\n",
    "        if len(trimmed_audio) == len(audio):\n",
    "            plt.plot(audio)\n",
    "            plt.show()\n",
    "            print(audio)\n",
    "            plt.plot(trimmed_audio)\n",
    "            plt.show()\n",
    "        segment_length = len(trimmed_audio) // num_parts\n",
    "        segments = []\n",
    "\n",
    "        for i in range(num_parts):\n",
    "            start = i * segment_length\n",
    "            end = (i + 1) * segment_length\n",
    "            segment = trimmed_audio[start:end]\n",
    "            #print(\"start\", start)\n",
    "            #print(\"end\", end)\n",
    "            \n",
    "            # Si el segmento es demasiado corto, rellenar con ceros\n",
    "            if len(segment) < segment_length:\n",
    "                segment = np.pad(segment, (0, segment_length - len(segment)), 'constant')\n",
    "            \n",
    "            segments.append(segment)\n",
    "        \n",
    "        return segments\n",
    "\n",
    "\n",
    "    def process_dataset(self, data_path, save_path, n_segments=1):\n",
    "\n",
    "        files = glob(data_path)\n",
    "        #files = [data_path]\n",
    "\n",
    "        smile = opensmile.Smile(\n",
    "            feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
    "            feature_level=opensmile.FeatureLevel.Functionals,\n",
    "        )\n",
    "\n",
    "        features_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        for file in files:\n",
    "            # Cargar y segmentar el audio\n",
    "            audio, sr = sf.read(file)\n",
    "            print(file)\n",
    "            segments = self.segment_audio2(audio, n_segments) ## CAMBIO sacar el 2\n",
    "\n",
    "            for segment in segments:\n",
    "                #print(len(segment))\n",
    "                segment_file = 'temp_segment.wav'\n",
    "                sf.write(segment_file, segment, sr)\n",
    "                features = smile.process_file(segment_file)\n",
    "                features_list.append(features.values.flatten())\n",
    "                labels_list.append(int(os.path.basename(file).split('-')[2]))\n",
    "\n",
    "        x_ = np.array(features_list)\n",
    "        y_ = np.array(labels_list)\n",
    "        \n",
    "        y_reshaped = y_[:, np.newaxis]\n",
    "        dataset = np.concatenate((x_, y_reshaped), axis=1)\n",
    "\n",
    "        actors = np.array([int(os.path.dirname(path)[-2:]) for path in files])\n",
    "        np.save(save_path, dataset)\n",
    "        print('guardo el dataset en,', save_path)\n",
    "        actors_save_path = define_actors_path(save_path)\n",
    "        np.save(actors_save_path, actors)\n",
    "\n",
    "        return np.load(save_path)\n",
    "\n",
    "    def get_dataset(self, dataset_path_list):\n",
    "        n_segments = int(os.path.basename(dataset_path_list[0])[0])\n",
    "        dataset = np.load(dataset_path_list[0])\n",
    "        actors_path = define_actors_path(dataset_path_list[0])\n",
    "        print(actors_path)\n",
    "        actors = np.load(actors_path)\n",
    "        \n",
    "        for i in range(1, len(dataset_path_list)):\n",
    "            dataset = np.concatenate((dataset, np.load(dataset_path_list[i])))\n",
    "            actors_path = actors_path = define_actors_path(dataset_path_list[i])\n",
    "            actors = np.concatenate((actors, np.load(actors_path)))\n",
    "\n",
    "        # Extraer características (x) y etiquetas (y)\n",
    "        x = dataset[:, :-1]\n",
    "        y = dataset[:, -1]\n",
    "        # Realizar reshape de x\n",
    "        num_samples = x.shape[0] // n_segments\n",
    "        num_features = x.shape[1]\n",
    "        x = x.reshape(num_samples, n_segments, num_features)\n",
    "        \n",
    "        # Realizar reshape de y\n",
    "        y = y.reshape(num_samples, n_segments).mean(axis=1).astype(int)\n",
    "        \n",
    "        # Ajustar actores al nuevo número de muestras\n",
    "        #actors_reshaped = actors.reshape(num_samples, n_segments).mean(axis=1).astype(int)\n",
    "\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        print(actors.shape)\n",
    "        return x, y, actors\n",
    "\n",
    "    def split_dataset(self, x, y, test_size = 0.2, actors=[]):\n",
    "        if len(actors) > 0:\n",
    "            gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "            train_idx, test_idx = next(gss.split(x, y, actors))\n",
    "            X_train, X_test = x[train_idx], x[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            actors_train = actors[train_idx]\n",
    "            print(\"ACTORES\", len(np.unique(actors_train)))\n",
    "            actors_test = actors[test_idx]\n",
    "\n",
    "            train_shuffle_idx = np.random.permutation(len(X_train))\n",
    "            test_shuffle_idx = np.random.permutation(len(X_test))\n",
    "            \n",
    "            X_train, y_train, actors_train = X_train[train_shuffle_idx], y_train[train_shuffle_idx], actors_train[train_shuffle_idx]\n",
    "            X_test, y_test, actors_test = X_test[test_shuffle_idx], y_test[test_shuffle_idx], actors_test[test_shuffle_idx]\n",
    "\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=42)\n",
    "            actors_train = None\n",
    "            actors_test = None\n",
    "\n",
    "        return X_train, X_test, y_train, y_test, actors_train, actors_test\n",
    "    \n",
    "def define_actors_path(dataset_path):\n",
    "    file_name = os.path.basename(dataset_path).split('_')\n",
    "    file_name[-1] = file_name[-1].split('.')[0]\n",
    "    actors_save_path = f\"data/{file_name[0]}_{file_name[1]}_actors_{file_name[3]}.npy\"\n",
    "    return actors_save_path\n",
    "\n",
    "\n",
    "AP = AudioProcessor()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se inicializo\n"
     ]
    }
   ],
   "source": [
    "DL = DataLoader()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\Audio_Speech_Actors_01-24\\\\Actor_09\\\\03-01-02-01-02-01-09(1).wav'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech1_dataset_dev = \"data/1part_Speech_dataset_dev.npy\"\n",
    "speech2_dataset_dev = \"data/2part_Speech_dataset_dev.npy\"\n",
    "speech4_dataset_dev = \"data/4part_Speech_dataset_dev.npy\"\n",
    "speech8_dataset_dev = \"data/8part_Speech_dataset_dev.npy\"\n",
    "\n",
    "speech1_dataset_test = \"data/1part_Speech_dataset_test.npy\"\n",
    "speech2_dataset_test = \"data/2part_Speech_dataset_test.npy\"\n",
    "speech4_dataset_test = \"data/4part_Speech_dataset_test.npy\"\n",
    "speech8_dataset_test = \"data/8part_Speech_dataset_test.npy\"\n",
    "\n",
    "path_dev = r\"data\\Audio_Speech_Actors_01-24\\*\\*\"\n",
    "path_test = r\"data/data_test/speech/*/*\"\n",
    "\n",
    "\n",
    "\"data\\Audio_Speech_Actors_01-24\\Actor_09\\03-01-04-01-01-01-09.wav\"\n",
    "r\"data\\Audio_Speech_Actors_01-24\\Actor_09\\03-01-02-01-02-01-09(1).wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL.process_dataset(path_dev, speech4_dataset_dev, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/4part_Speech_actors_dev.npy\n",
      "X shape (1156, 4, 88)\n",
      "y shape (1156,)\n",
      "actors shape (1156,)\n",
      "data/4part_Speech_actors_test.npy\n",
      "X shape (300, 4, 88)\n",
      "y shape (300,)\n",
      "actors shape (300,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_dev, y_dev, actors_dev = DL.get_dataset([speech4_dataset_dev])\n",
    "X_train, X_valid, y_train, y_valid, actors_train, actors_valid = DL.split_dataset(X_dev, y_dev, test_size=0.2, actors=actors_dev)\n",
    "X_test, y_test, actors_test = DL.get_dataset([speech4_dataset_test])\n",
    "\n",
    "plt.hist(y_train)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "X_train = normalization(X_train)\n",
    "X_valid = normalization(X_valid)\n",
    "X_test = normalization(X_test)\n",
    "\n",
    "y_train_ohe  = one_hot_encoder(y_train)\n",
    "y_valid_ohe = one_hot_encoder(y_valid)\n",
    "y_test_ohe = one_hot_encoder(y_test)\n",
    "y_dev_ohe = one_hot_encoder(y_dev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM con keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_validate_rnn(X, y, groups, n_splits=5, epochs=10, batch_size=32):\n",
    "    valid_loss = []\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    fold_no = 1\n",
    "    for train_index, val_index in gkf.split(X, y, groups):\n",
    "        print(f'Training fold {fold_no} ...')\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        model = lstm.rnnLSTM(X_train, y_train)\n",
    "        model.train(X_train, y_train, X_val, y_val, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "        val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "        print(f'Fold {fold_no} - Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "        valid_loss.append(val_loss)\n",
    "        fold_no += 1\n",
    "    return np.mean(valid_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1 ...\n",
      "Epoch 1/10\n",
      "57/57 [==============================] - 5s 26ms/step - loss: 2.0983 - accuracy: 0.1264 - val_loss: 2.0606 - val_accuracy: 0.1260\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 2.0707 - accuracy: 0.1440 - val_loss: 2.0423 - val_accuracy: 0.1585\n",
      "Epoch 3/10\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 2.0473 - accuracy: 0.1626 - val_loss: 2.0399 - val_accuracy: 0.1504\n",
      "Epoch 4/10\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 2.0528 - accuracy: 0.1440 - val_loss: 2.0310 - val_accuracy: 0.1545\n",
      "Epoch 5/10\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 2.0489 - accuracy: 0.1341 - val_loss: 2.0259 - val_accuracy: 0.2154\n",
      "Epoch 6/10\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 2.0402 - accuracy: 0.1538 - val_loss: 2.0256 - val_accuracy: 0.1951\n",
      "Epoch 7/10\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 2.0335 - accuracy: 0.1813 - val_loss: 2.0230 - val_accuracy: 0.1545\n",
      "Epoch 8/10\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 2.0538 - accuracy: 0.1396 - val_loss: 2.0329 - val_accuracy: 0.1545\n",
      "Epoch 9/10\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 2.0354 - accuracy: 0.1549 - val_loss: 2.0199 - val_accuracy: 0.2114\n",
      "Epoch 10/10\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 2.0271 - accuracy: 0.1835 - val_loss: 2.0210 - val_accuracy: 0.2195\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.0210 - accuracy: 0.2195\n",
      "Fold 1 - Validation Loss: 2.0210, Validation Accuracy: 0.2195\n",
      "Training fold 2 ...\n",
      "Epoch 1/10\n",
      "61/61 [==============================] - 4s 20ms/step - loss: 2.0885 - accuracy: 0.1309 - val_loss: 2.0457 - val_accuracy: 0.1720\n",
      "Epoch 2/10\n",
      "61/61 [==============================] - 1s 9ms/step - loss: 2.0751 - accuracy: 0.1351 - val_loss: 2.0402 - val_accuracy: 0.1774\n",
      "Epoch 3/10\n",
      "61/61 [==============================] - 1s 8ms/step - loss: 2.0629 - accuracy: 0.1454 - val_loss: 2.0112 - val_accuracy: 0.2043\n",
      "Epoch 4/10\n",
      "61/61 [==============================] - 1s 9ms/step - loss: 2.0615 - accuracy: 0.1433 - val_loss: 1.9902 - val_accuracy: 0.2258\n",
      "Epoch 5/10\n",
      "61/61 [==============================] - 1s 9ms/step - loss: 2.0526 - accuracy: 0.1526 - val_loss: 1.9821 - val_accuracy: 0.2097\n",
      "Epoch 6/10\n",
      "61/61 [==============================] - 1s 9ms/step - loss: 2.0313 - accuracy: 0.1804 - val_loss: 1.9323 - val_accuracy: 0.2849\n",
      "Epoch 7/10\n",
      "61/61 [==============================] - 1s 9ms/step - loss: 2.0125 - accuracy: 0.1701 - val_loss: 1.9312 - val_accuracy: 0.2151\n",
      "Epoch 8/10\n",
      "61/61 [==============================] - 1s 10ms/step - loss: 2.0035 - accuracy: 0.2072 - val_loss: 1.8681 - val_accuracy: 0.2903\n",
      "Epoch 9/10\n",
      "61/61 [==============================] - 1s 10ms/step - loss: 2.0116 - accuracy: 0.1897 - val_loss: 1.9138 - val_accuracy: 0.2903\n",
      "Epoch 10/10\n",
      "61/61 [==============================] - 1s 9ms/step - loss: 2.0141 - accuracy: 0.1711 - val_loss: 1.9155 - val_accuracy: 0.2688\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9155 - accuracy: 0.2688\n",
      "Fold 2 - Validation Loss: 1.9155, Validation Accuracy: 0.2688\n",
      "Training fold 3 ...\n",
      "Epoch 1/10\n",
      "57/57 [==============================] - 4s 20ms/step - loss: 2.0836 - accuracy: 0.1404 - val_loss: 2.0561 - val_accuracy: 0.1516\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 2.0615 - accuracy: 0.1623 - val_loss: 2.0567 - val_accuracy: 0.1434\n",
      "Epoch 3/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 2.0417 - accuracy: 0.1656 - val_loss: 2.0419 - val_accuracy: 0.1434\n",
      "Epoch 4/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 2.0423 - accuracy: 0.1601 - val_loss: 2.0317 - val_accuracy: 0.2254\n",
      "Epoch 5/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 1.9972 - accuracy: 0.2204 - val_loss: 2.0122 - val_accuracy: 0.1803\n",
      "Epoch 6/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 1.9660 - accuracy: 0.2171 - val_loss: 2.0156 - val_accuracy: 0.1803\n",
      "Epoch 7/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 1.9787 - accuracy: 0.1996 - val_loss: 1.9787 - val_accuracy: 0.1885\n",
      "Epoch 8/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 1.9922 - accuracy: 0.1864 - val_loss: 2.0629 - val_accuracy: 0.1680\n",
      "Epoch 9/10\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 1.9626 - accuracy: 0.2083 - val_loss: 2.0032 - val_accuracy: 0.1885\n",
      "Epoch 10/10\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 1.9548 - accuracy: 0.2281 - val_loss: 2.0214 - val_accuracy: 0.1967\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.0214 - accuracy: 0.1967\n",
      "Fold 3 - Validation Loss: 2.0214, Validation Accuracy: 0.1967\n",
      "Training fold 4 ...\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 5s 21ms/step - loss: 2.0936 - accuracy: 0.1332 - val_loss: 2.0530 - val_accuracy: 0.1417\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 2.0752 - accuracy: 0.1496 - val_loss: 2.0528 - val_accuracy: 0.1708\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 2.0669 - accuracy: 0.1430 - val_loss: 2.0347 - val_accuracy: 0.1833\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 0s 8ms/step - loss: 2.0483 - accuracy: 0.1605 - val_loss: 2.0313 - val_accuracy: 0.1750\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 0s 9ms/step - loss: 2.0340 - accuracy: 0.1659 - val_loss: 1.9945 - val_accuracy: 0.2583\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 0s 8ms/step - loss: 2.0491 - accuracy: 0.1627 - val_loss: 2.0133 - val_accuracy: 0.1750\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 0s 8ms/step - loss: 2.0299 - accuracy: 0.2031 - val_loss: 2.0079 - val_accuracy: 0.1542\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 2.0253 - accuracy: 0.1769 - val_loss: 1.9688 - val_accuracy: 0.2292\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 0s 8ms/step - loss: 2.0180 - accuracy: 0.1769 - val_loss: 2.0041 - val_accuracy: 0.2167\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - 0s 8ms/step - loss: 1.9973 - accuracy: 0.1976 - val_loss: 1.9483 - val_accuracy: 0.1917\n",
      "8/8 [==============================] - 1s 4ms/step - loss: 1.9483 - accuracy: 0.1917\n",
      "Fold 4 - Validation Loss: 1.9483, Validation Accuracy: 0.1917\n",
      "Training fold 5 ...\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 4s 19ms/step - loss: 2.0859 - accuracy: 0.1430 - val_loss: 2.0599 - val_accuracy: 0.1417\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 0s 8ms/step - loss: 2.0814 - accuracy: 0.1376 - val_loss: 2.0539 - val_accuracy: 0.1667\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 2.0498 - accuracy: 0.1583 - val_loss: 2.0265 - val_accuracy: 0.1917\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 0s 8ms/step - loss: 2.0517 - accuracy: 0.1572 - val_loss: 2.0099 - val_accuracy: 0.2458\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 0s 8ms/step - loss: 2.0254 - accuracy: 0.1736 - val_loss: 1.9690 - val_accuracy: 0.2458\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 0s 8ms/step - loss: 1.9958 - accuracy: 0.2096 - val_loss: 1.9740 - val_accuracy: 0.2208\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 0s 8ms/step - loss: 1.9894 - accuracy: 0.1943 - val_loss: 1.9650 - val_accuracy: 0.2042\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 1.9814 - accuracy: 0.2063 - val_loss: 1.9309 - val_accuracy: 0.2875\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 0s 8ms/step - loss: 1.9867 - accuracy: 0.1910 - val_loss: 1.9676 - val_accuracy: 0.1792\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - 0s 8ms/step - loss: 1.9787 - accuracy: 0.2107 - val_loss: 1.9427 - val_accuracy: 0.2542\n",
      "8/8 [==============================] - 1s 4ms/step - loss: 1.9427 - accuracy: 0.2542\n",
      "Fold 5 - Validation Loss: 1.9427, Validation Accuracy: 0.2542\n",
      "Training fold 1 ...\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 4s 34ms/step - loss: 2.1037 - accuracy: 0.1484 - val_loss: 2.0458 - val_accuracy: 0.2114\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 2.0618 - accuracy: 0.1571 - val_loss: 2.0315 - val_accuracy: 0.1951\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 2.0596 - accuracy: 0.1473 - val_loss: 2.0260 - val_accuracy: 0.1992\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 2.0318 - accuracy: 0.1736 - val_loss: 2.0188 - val_accuracy: 0.1585\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 2.0372 - accuracy: 0.1736 - val_loss: 2.0161 - val_accuracy: 0.1911\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 2.0103 - accuracy: 0.1956 - val_loss: 1.9876 - val_accuracy: 0.1911\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.9917 - accuracy: 0.1945 - val_loss: 1.9660 - val_accuracy: 0.1748\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.9668 - accuracy: 0.2176 - val_loss: 1.9516 - val_accuracy: 0.1789\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 1.9783 - accuracy: 0.2099 - val_loss: 1.9402 - val_accuracy: 0.1911\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.9288 - accuracy: 0.2253 - val_loss: 1.9361 - val_accuracy: 0.1951\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.9361 - accuracy: 0.1951\n",
      "Fold 1 - Validation Loss: 1.9361, Validation Accuracy: 0.1951\n",
      "Training fold 2 ...\n",
      "Epoch 1/10\n",
      "31/31 [==============================] - 4s 33ms/step - loss: 2.1031 - accuracy: 0.1330 - val_loss: 2.0286 - val_accuracy: 0.2527\n",
      "Epoch 2/10\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 2.0570 - accuracy: 0.1711 - val_loss: 1.9884 - val_accuracy: 0.2957\n",
      "Epoch 3/10\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 2.0353 - accuracy: 0.1825 - val_loss: 1.9648 - val_accuracy: 0.2634\n",
      "Epoch 4/10\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 2.0447 - accuracy: 0.1629 - val_loss: 1.9270 - val_accuracy: 0.3011\n",
      "Epoch 5/10\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 2.0068 - accuracy: 0.1948 - val_loss: 1.9160 - val_accuracy: 0.2903\n",
      "Epoch 6/10\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 2.0174 - accuracy: 0.1804 - val_loss: 1.8937 - val_accuracy: 0.2796\n",
      "Epoch 7/10\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 1.9873 - accuracy: 0.2010 - val_loss: 1.8868 - val_accuracy: 0.2849\n",
      "Epoch 8/10\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 1.9833 - accuracy: 0.1938 - val_loss: 1.8763 - val_accuracy: 0.2903\n",
      "Epoch 9/10\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 1.9590 - accuracy: 0.1948 - val_loss: 1.8497 - val_accuracy: 0.2957\n",
      "Epoch 10/10\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 1.9434 - accuracy: 0.2103 - val_loss: 1.8349 - val_accuracy: 0.2849\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8349 - accuracy: 0.2849\n",
      "Fold 2 - Validation Loss: 1.8349, Validation Accuracy: 0.2849\n",
      "Training fold 3 ...\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 5s 47ms/step - loss: 2.0797 - accuracy: 0.1458 - val_loss: 2.0560 - val_accuracy: 0.1762\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 2.0556 - accuracy: 0.1579 - val_loss: 2.0516 - val_accuracy: 0.1680\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 2.0416 - accuracy: 0.1689 - val_loss: 2.0451 - val_accuracy: 0.1762\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 2.0133 - accuracy: 0.1897 - val_loss: 2.0204 - val_accuracy: 0.1639\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.9799 - accuracy: 0.2127 - val_loss: 2.0126 - val_accuracy: 0.2131\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.9684 - accuracy: 0.2105 - val_loss: 1.9948 - val_accuracy: 0.2336\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 1.9537 - accuracy: 0.2226 - val_loss: 1.9683 - val_accuracy: 0.1885\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.9563 - accuracy: 0.2039 - val_loss: 1.9974 - val_accuracy: 0.2295\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.9395 - accuracy: 0.2281 - val_loss: 1.9446 - val_accuracy: 0.2336\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.9263 - accuracy: 0.2412 - val_loss: 1.9687 - val_accuracy: 0.2049\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.9687 - accuracy: 0.2049\n",
      "Fold 3 - Validation Loss: 1.9687, Validation Accuracy: 0.2049\n",
      "Training fold 4 ...\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 5s 41ms/step - loss: 2.0857 - accuracy: 0.1474 - val_loss: 2.0528 - val_accuracy: 0.1250\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 2.0743 - accuracy: 0.1496 - val_loss: 2.0395 - val_accuracy: 0.2083\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 2.0420 - accuracy: 0.1747 - val_loss: 2.0089 - val_accuracy: 0.1833\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 2.0276 - accuracy: 0.1779 - val_loss: 1.9705 - val_accuracy: 0.2458\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 2.0060 - accuracy: 0.1834 - val_loss: 1.9612 - val_accuracy: 0.2417\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 2.0083 - accuracy: 0.1987 - val_loss: 1.9238 - val_accuracy: 0.2375\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.9797 - accuracy: 0.2020 - val_loss: 1.9415 - val_accuracy: 0.1958\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.9722 - accuracy: 0.2096 - val_loss: 1.9109 - val_accuracy: 0.2458\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.9686 - accuracy: 0.2151 - val_loss: 1.9177 - val_accuracy: 0.2292\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.9582 - accuracy: 0.2129 - val_loss: 1.8918 - val_accuracy: 0.2667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.8918 - accuracy: 0.2667\n",
      "Fold 4 - Validation Loss: 1.8918, Validation Accuracy: 0.2667\n",
      "Training fold 5 ...\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 4s 35ms/step - loss: 2.0811 - accuracy: 0.1561 - val_loss: 2.0587 - val_accuracy: 0.1625\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 2.0702 - accuracy: 0.1583 - val_loss: 2.0588 - val_accuracy: 0.1208\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 2.0583 - accuracy: 0.1397 - val_loss: 2.0322 - val_accuracy: 0.2250\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 2.0401 - accuracy: 0.1736 - val_loss: 2.0206 - val_accuracy: 0.2375\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 2.0489 - accuracy: 0.1616 - val_loss: 2.0105 - val_accuracy: 0.1917\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 2.0090 - accuracy: 0.1921 - val_loss: 1.9749 - val_accuracy: 0.2167\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.9979 - accuracy: 0.1976 - val_loss: 1.9507 - val_accuracy: 0.2583\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.9643 - accuracy: 0.2096 - val_loss: 1.9499 - val_accuracy: 0.2333\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.9585 - accuracy: 0.2041 - val_loss: 1.9624 - val_accuracy: 0.2250\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.9763 - accuracy: 0.2194 - val_loss: 1.9271 - val_accuracy: 0.2292\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.9271 - accuracy: 0.2292\n",
      "Fold 5 - Validation Loss: 1.9271, Validation Accuracy: 0.2292\n",
      "Training fold 1 ...\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 4s 78ms/step - loss: 2.0921 - accuracy: 0.1418 - val_loss: 2.0549 - val_accuracy: 0.1667\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 2.0833 - accuracy: 0.1407 - val_loss: 2.0549 - val_accuracy: 0.1138\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 2.0822 - accuracy: 0.1231 - val_loss: 2.0493 - val_accuracy: 0.1463\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 2.0520 - accuracy: 0.1549 - val_loss: 2.0378 - val_accuracy: 0.2073\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 2.0606 - accuracy: 0.1527 - val_loss: 2.0319 - val_accuracy: 0.1748\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 2.0503 - accuracy: 0.1670 - val_loss: 2.0205 - val_accuracy: 0.2033\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 2.0382 - accuracy: 0.1824 - val_loss: 2.0035 - val_accuracy: 0.1667\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 2.0176 - accuracy: 0.1901 - val_loss: 1.9857 - val_accuracy: 0.1992\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 2.0016 - accuracy: 0.1725 - val_loss: 1.9693 - val_accuracy: 0.1911\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9928 - accuracy: 0.2132 - val_loss: 1.9485 - val_accuracy: 0.1951\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.9485 - accuracy: 0.1951\n",
      "Fold 1 - Validation Loss: 1.9485, Validation Accuracy: 0.1951\n",
      "Training fold 2 ...\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 4s 59ms/step - loss: 2.0928 - accuracy: 0.1412 - val_loss: 2.0560 - val_accuracy: 0.1774\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.0763 - accuracy: 0.1474 - val_loss: 2.0382 - val_accuracy: 0.1935\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.0700 - accuracy: 0.1392 - val_loss: 2.0167 - val_accuracy: 0.2634\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.0405 - accuracy: 0.1588 - val_loss: 1.9809 - val_accuracy: 0.2796\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.0319 - accuracy: 0.1649 - val_loss: 1.9342 - val_accuracy: 0.2849\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.0055 - accuracy: 0.1814 - val_loss: 1.8924 - val_accuracy: 0.3011\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.9814 - accuracy: 0.2072 - val_loss: 1.8568 - val_accuracy: 0.3065\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.9867 - accuracy: 0.1938 - val_loss: 1.8327 - val_accuracy: 0.2957\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.9641 - accuracy: 0.2165 - val_loss: 1.8245 - val_accuracy: 0.2957\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.9689 - accuracy: 0.2041 - val_loss: 1.8648 - val_accuracy: 0.2849\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8648 - accuracy: 0.2849\n",
      "Fold 2 - Validation Loss: 1.8648, Validation Accuracy: 0.2849\n",
      "Training fold 3 ...\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 4s 69ms/step - loss: 2.0997 - accuracy: 0.1283 - val_loss: 2.0551 - val_accuracy: 0.1475\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 2.0674 - accuracy: 0.1546 - val_loss: 2.0479 - val_accuracy: 0.1844\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 2.0507 - accuracy: 0.1743 - val_loss: 2.0411 - val_accuracy: 0.1803\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 2.0263 - accuracy: 0.1787 - val_loss: 2.0304 - val_accuracy: 0.1762\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 2.0180 - accuracy: 0.1897 - val_loss: 2.0221 - val_accuracy: 0.2008\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 2.0015 - accuracy: 0.1864 - val_loss: 2.0276 - val_accuracy: 0.1311\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9980 - accuracy: 0.2116 - val_loss: 2.0201 - val_accuracy: 0.1967\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9843 - accuracy: 0.2018 - val_loss: 2.0060 - val_accuracy: 0.1598\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9455 - accuracy: 0.2237 - val_loss: 2.0138 - val_accuracy: 0.2008\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9634 - accuracy: 0.2314 - val_loss: 2.0019 - val_accuracy: 0.1844\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.0019 - accuracy: 0.1844\n",
      "Fold 3 - Validation Loss: 2.0019, Validation Accuracy: 0.1844\n",
      "Training fold 4 ...\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 4s 67ms/step - loss: 2.1042 - accuracy: 0.1376 - val_loss: 2.0591 - val_accuracy: 0.1750\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 2.0621 - accuracy: 0.1605 - val_loss: 2.0263 - val_accuracy: 0.3125\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 2.0485 - accuracy: 0.1769 - val_loss: 2.0187 - val_accuracy: 0.2292\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 2.0383 - accuracy: 0.1659 - val_loss: 1.9971 - val_accuracy: 0.2250\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 2.0271 - accuracy: 0.1823 - val_loss: 1.9595 - val_accuracy: 0.2500\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 2.0019 - accuracy: 0.2041 - val_loss: 1.9411 - val_accuracy: 0.2333\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9927 - accuracy: 0.2118 - val_loss: 1.9071 - val_accuracy: 0.2542\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9687 - accuracy: 0.2249 - val_loss: 1.9049 - val_accuracy: 0.2292\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9542 - accuracy: 0.2151 - val_loss: 1.8797 - val_accuracy: 0.2625\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9388 - accuracy: 0.2172 - val_loss: 1.8661 - val_accuracy: 0.2583\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.8661 - accuracy: 0.2583\n",
      "Fold 4 - Validation Loss: 1.8661, Validation Accuracy: 0.2583\n",
      "Training fold 5 ...\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 4s 60ms/step - loss: 2.0891 - accuracy: 0.1496 - val_loss: 2.0490 - val_accuracy: 0.1583\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 2.0670 - accuracy: 0.1605 - val_loss: 2.0409 - val_accuracy: 0.2292\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 2.0586 - accuracy: 0.1616 - val_loss: 2.0337 - val_accuracy: 0.2625\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 2.0471 - accuracy: 0.1463 - val_loss: 2.0159 - val_accuracy: 0.2042\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 2.0361 - accuracy: 0.1736 - val_loss: 1.9964 - val_accuracy: 0.2458\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 2.0057 - accuracy: 0.2009 - val_loss: 1.9591 - val_accuracy: 0.2875\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9999 - accuracy: 0.1965 - val_loss: 1.9512 - val_accuracy: 0.2292\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9860 - accuracy: 0.2031 - val_loss: 1.9579 - val_accuracy: 0.2833\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9773 - accuracy: 0.2118 - val_loss: 1.9399 - val_accuracy: 0.2625\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9504 - accuracy: 0.2260 - val_loss: 1.9069 - val_accuracy: 0.2542\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.9069 - accuracy: 0.2542\n",
      "Fold 5 - Validation Loss: 1.9069, Validation Accuracy: 0.2542\n",
      "Training fold 1 ...\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 5s 128ms/step - loss: 2.0941 - accuracy: 0.1297 - val_loss: 2.0648 - val_accuracy: 0.1748\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 2.0791 - accuracy: 0.1352 - val_loss: 2.0601 - val_accuracy: 0.1748\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 2.0805 - accuracy: 0.1319 - val_loss: 2.0513 - val_accuracy: 0.1992\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 2.0687 - accuracy: 0.1495 - val_loss: 2.0432 - val_accuracy: 0.1911\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 2.0558 - accuracy: 0.1670 - val_loss: 2.0346 - val_accuracy: 0.1951\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 2.0482 - accuracy: 0.1637 - val_loss: 2.0218 - val_accuracy: 0.1870\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 2.0452 - accuracy: 0.1582 - val_loss: 2.0145 - val_accuracy: 0.2033\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 2.0251 - accuracy: 0.1769 - val_loss: 2.0060 - val_accuracy: 0.1951\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 2.0132 - accuracy: 0.1813 - val_loss: 2.0061 - val_accuracy: 0.1829\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 1.9974 - accuracy: 0.2022 - val_loss: 1.9963 - val_accuracy: 0.1870\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.9963 - accuracy: 0.1870\n",
      "Fold 1 - Validation Loss: 1.9963, Validation Accuracy: 0.1870\n",
      "Training fold 2 ...\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 4s 156ms/step - loss: 2.0892 - accuracy: 0.1381 - val_loss: 2.0360 - val_accuracy: 0.2742\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 2.0733 - accuracy: 0.1433 - val_loss: 2.0244 - val_accuracy: 0.2581\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 2.0673 - accuracy: 0.1433 - val_loss: 2.0096 - val_accuracy: 0.2366\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 2.0541 - accuracy: 0.1515 - val_loss: 2.0019 - val_accuracy: 0.2204\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 2.0466 - accuracy: 0.1753 - val_loss: 1.9893 - val_accuracy: 0.2419\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 2.0446 - accuracy: 0.1866 - val_loss: 1.9707 - val_accuracy: 0.2742\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 2.0203 - accuracy: 0.2093 - val_loss: 1.9420 - val_accuracy: 0.2849\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 2.0148 - accuracy: 0.1897 - val_loss: 1.9277 - val_accuracy: 0.2688\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 2.0049 - accuracy: 0.1918 - val_loss: 1.9063 - val_accuracy: 0.2419\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 1.9732 - accuracy: 0.2093 - val_loss: 1.8863 - val_accuracy: 0.2849\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8863 - accuracy: 0.2849\n",
      "Fold 2 - Validation Loss: 1.8863, Validation Accuracy: 0.2849\n",
      "Training fold 3 ...\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 5s 121ms/step - loss: 2.1219 - accuracy: 0.1162 - val_loss: 2.0792 - val_accuracy: 0.1066\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 2.0788 - accuracy: 0.1524 - val_loss: 2.0676 - val_accuracy: 0.1475\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 2.0632 - accuracy: 0.1546 - val_loss: 2.0611 - val_accuracy: 0.1311\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 2.0533 - accuracy: 0.1480 - val_loss: 2.0530 - val_accuracy: 0.1516\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 2.0405 - accuracy: 0.1645 - val_loss: 2.0468 - val_accuracy: 0.1557\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 2.0248 - accuracy: 0.1678 - val_loss: 2.0479 - val_accuracy: 0.1475\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 2.0280 - accuracy: 0.1809 - val_loss: 2.0444 - val_accuracy: 0.1475\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 2.0145 - accuracy: 0.1831 - val_loss: 2.0386 - val_accuracy: 0.1639\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 1.9830 - accuracy: 0.1930 - val_loss: 2.0440 - val_accuracy: 0.1803\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1.9828 - accuracy: 0.2029 - val_loss: 2.0285 - val_accuracy: 0.1680\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.0285 - accuracy: 0.1680\n",
      "Fold 3 - Validation Loss: 2.0285, Validation Accuracy: 0.1680\n",
      "Training fold 4 ...\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 4s 113ms/step - loss: 2.1041 - accuracy: 0.1255 - val_loss: 2.0617 - val_accuracy: 0.1333\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 2.0799 - accuracy: 0.1430 - val_loss: 2.0465 - val_accuracy: 0.1667\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 2.0604 - accuracy: 0.1496 - val_loss: 2.0294 - val_accuracy: 0.2292\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 2.0569 - accuracy: 0.1561 - val_loss: 2.0243 - val_accuracy: 0.2208\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 2.0387 - accuracy: 0.1714 - val_loss: 2.0041 - val_accuracy: 0.2375\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 2.0307 - accuracy: 0.1889 - val_loss: 1.9846 - val_accuracy: 0.2792\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 2.0167 - accuracy: 0.1627 - val_loss: 1.9674 - val_accuracy: 0.2167\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 1.9958 - accuracy: 0.2031 - val_loss: 1.9314 - val_accuracy: 0.2667\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 1.9638 - accuracy: 0.2293 - val_loss: 1.8993 - val_accuracy: 0.2792\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 1.9592 - accuracy: 0.2260 - val_loss: 1.8855 - val_accuracy: 0.2625\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.8855 - accuracy: 0.2625\n",
      "Fold 4 - Validation Loss: 1.8855, Validation Accuracy: 0.2625\n",
      "Training fold 5 ...\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 4s 123ms/step - loss: 2.1079 - accuracy: 0.1201 - val_loss: 2.0676 - val_accuracy: 0.1292\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 2.0983 - accuracy: 0.1397 - val_loss: 2.0532 - val_accuracy: 0.1500\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 2.0706 - accuracy: 0.1517 - val_loss: 2.0372 - val_accuracy: 0.1875\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 2.0628 - accuracy: 0.1419 - val_loss: 2.0296 - val_accuracy: 0.2250\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 2.0479 - accuracy: 0.1638 - val_loss: 2.0183 - val_accuracy: 0.2167\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 2.0381 - accuracy: 0.1714 - val_loss: 2.0041 - val_accuracy: 0.2250\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 2.0234 - accuracy: 0.1725 - val_loss: 1.9877 - val_accuracy: 0.2500\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 2.0179 - accuracy: 0.2074 - val_loss: 1.9787 - val_accuracy: 0.2417\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 1.9971 - accuracy: 0.1921 - val_loss: 1.9515 - val_accuracy: 0.2417\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 1.9925 - accuracy: 0.2031 - val_loss: 1.9338 - val_accuracy: 0.2417\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.9338 - accuracy: 0.2417\n",
      "Fold 5 - Validation Loss: 1.9338, Validation Accuracy: 0.2417\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batches_size = [16, 32, 64, 128]\n",
    "valid_loss = []\n",
    "for batch_s in batches_size:\n",
    "    loss = cross_validate_rnn(X_dev, y_dev_ohe, actors_dev, batch_size=batch_s)\n",
    "    valid_loss.append(loss)\n",
    "plt.plot(batches_size, valid_loss)\n",
    "plt.title(\"Loss para distintos batches\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Training fold 1 ...\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 4s 40ms/step - loss: 2.0208 - accuracy: 0.2008 - val_loss: 1.9573 - val_accuracy: 0.2717\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.8434 - accuracy: 0.2650 - val_loss: 1.8116 - val_accuracy: 0.2663\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.6929 - accuracy: 0.3484 - val_loss: 1.6876 - val_accuracy: 0.3261\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5510 - accuracy: 0.4153 - val_loss: 1.5621 - val_accuracy: 0.3641\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4342 - accuracy: 0.4454 - val_loss: 1.4969 - val_accuracy: 0.4022\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3631 - accuracy: 0.4891 - val_loss: 1.4251 - val_accuracy: 0.4348\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.2315 - accuracy: 0.5533 - val_loss: 1.4314 - val_accuracy: 0.4511\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.1263 - accuracy: 0.6038 - val_loss: 1.3750 - val_accuracy: 0.4511\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.0624 - accuracy: 0.6243 - val_loss: 1.4123 - val_accuracy: 0.4783\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.0037 - accuracy: 0.6503 - val_loss: 1.3783 - val_accuracy: 0.4891\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.3783 - accuracy: 0.4891\n",
      "Fold 1 - Validation Loss: 1.3783, Validation Accuracy: 0.4891\n",
      "Training fold 2 ...\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 4s 42ms/step - loss: 2.0111 - accuracy: 0.2224 - val_loss: 1.8966 - val_accuracy: 0.2623\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.8469 - accuracy: 0.2906 - val_loss: 1.7527 - val_accuracy: 0.2787\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.7365 - accuracy: 0.3288 - val_loss: 1.6284 - val_accuracy: 0.4044\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5742 - accuracy: 0.4175 - val_loss: 1.5236 - val_accuracy: 0.4317\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4764 - accuracy: 0.4461 - val_loss: 1.4337 - val_accuracy: 0.4863\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3583 - accuracy: 0.5020 - val_loss: 1.3854 - val_accuracy: 0.4809\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.2596 - accuracy: 0.5307 - val_loss: 1.3426 - val_accuracy: 0.5137\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.1675 - accuracy: 0.5662 - val_loss: 1.2940 - val_accuracy: 0.5246\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.0895 - accuracy: 0.6085 - val_loss: 1.2536 - val_accuracy: 0.5410\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.9630 - accuracy: 0.6617 - val_loss: 1.2335 - val_accuracy: 0.5355\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.2335 - accuracy: 0.5355\n",
      "Fold 2 - Validation Loss: 1.2335, Validation Accuracy: 0.5355\n",
      "Training fold 3 ...\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 4s 41ms/step - loss: 2.0416 - accuracy: 0.1637 - val_loss: 1.9293 - val_accuracy: 0.3825\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.8939 - accuracy: 0.2647 - val_loss: 1.7219 - val_accuracy: 0.3607\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.7597 - accuracy: 0.3124 - val_loss: 1.5997 - val_accuracy: 0.4372\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.6306 - accuracy: 0.3820 - val_loss: 1.4722 - val_accuracy: 0.4645\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5059 - accuracy: 0.4134 - val_loss: 1.3925 - val_accuracy: 0.4754\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3848 - accuracy: 0.4898 - val_loss: 1.3259 - val_accuracy: 0.5137\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.2867 - accuracy: 0.5034 - val_loss: 1.2856 - val_accuracy: 0.5082\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.2019 - accuracy: 0.5498 - val_loss: 1.2463 - val_accuracy: 0.5628\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.1192 - accuracy: 0.5866 - val_loss: 1.1876 - val_accuracy: 0.5519\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.0431 - accuracy: 0.6180 - val_loss: 1.2165 - val_accuracy: 0.5574\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2165 - accuracy: 0.5574\n",
      "Fold 3 - Validation Loss: 1.2165, Validation Accuracy: 0.5574\n",
      "Training fold 4 ...\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 4s 39ms/step - loss: 2.0329 - accuracy: 0.1910 - val_loss: 1.9221 - val_accuracy: 0.3388\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.8735 - accuracy: 0.2933 - val_loss: 1.7299 - val_accuracy: 0.3333\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.7599 - accuracy: 0.3438 - val_loss: 1.6228 - val_accuracy: 0.4044\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5961 - accuracy: 0.4052 - val_loss: 1.4778 - val_accuracy: 0.4645\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.5065 - accuracy: 0.4093 - val_loss: 1.3500 - val_accuracy: 0.5082\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3517 - accuracy: 0.5143 - val_loss: 1.3421 - val_accuracy: 0.5191\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.2328 - accuracy: 0.5512 - val_loss: 1.2537 - val_accuracy: 0.5355\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.1135 - accuracy: 0.6044 - val_loss: 1.2461 - val_accuracy: 0.5574\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.0454 - accuracy: 0.6521 - val_loss: 1.2126 - val_accuracy: 0.5355\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.9423 - accuracy: 0.6876 - val_loss: 1.2351 - val_accuracy: 0.5519\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.2351 - accuracy: 0.5519\n",
      "Fold 4 - Validation Loss: 1.2351, Validation Accuracy: 0.5519\n",
      "Training fold 5 ...\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 4s 40ms/step - loss: 2.0035 - accuracy: 0.2387 - val_loss: 1.9014 - val_accuracy: 0.2732\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.8365 - accuracy: 0.3056 - val_loss: 1.7423 - val_accuracy: 0.2896\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.6918 - accuracy: 0.3492 - val_loss: 1.6237 - val_accuracy: 0.3333\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5756 - accuracy: 0.3970 - val_loss: 1.5492 - val_accuracy: 0.3552\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.4597 - accuracy: 0.4584 - val_loss: 1.4583 - val_accuracy: 0.4208\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3662 - accuracy: 0.5034 - val_loss: 1.4077 - val_accuracy: 0.4372\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.2862 - accuracy: 0.5211 - val_loss: 1.3628 - val_accuracy: 0.4645\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.1930 - accuracy: 0.5648 - val_loss: 1.3227 - val_accuracy: 0.5137\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.1354 - accuracy: 0.5907 - val_loss: 1.2486 - val_accuracy: 0.5355\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.0926 - accuracy: 0.6057 - val_loss: 1.2427 - val_accuracy: 0.5464\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.2427 - accuracy: 0.5464\n",
      "Fold 5 - Validation Loss: 1.2427, Validation Accuracy: 0.5464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.2612184047698975"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"train\")\n",
    "\n",
    "\n",
    "y_train_ohe  = one_hot_encoder(y_train)\n",
    "y_valid_ohe = one_hot_encoder(y_valid)\n",
    "y_test_ohe = one_hot_encoder(y_test)\n",
    "\n",
    "R = lstm.rnnLSTM(X_train, y_train_ohe)\n",
    "R.train(X_train, y_train_ohe, X_valid, y_valid_ohe, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\")\n",
    "test_loss, test_accuracy = R.evaluate(X_test, y_test_ohe)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM con pythorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input_size = 3       # Número de características de entrada\\nhidden_size = 64     # Número de características en el estado oculto\\nnum_classes = 8      # Número de clases de salida\\nnum_layers = 1       # Número de capas de la LSTM\\nlearning_rate = 0.001\\nbatch_size = 32\\nnum_epochs = 50\\n\\nX_train_batch, y_train_batch = DL.create_batches(X_train, y_train, batch_size)\\nlstm_ = LSTM2.LSTMClassifier(input_size, hidden_size, num_classes, num_layers)\\n\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = optim.Adam(lstm_.parameters(), lr=learning_rate)\\nlstm_.train_model(X_train_batch, y_train_batch, criterion, optimizer, num_epochs=20)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"input_size = 3       # Número de características de entrada\n",
    "hidden_size = 64     # Número de características en el estado oculto\n",
    "num_classes = 8      # Número de clases de salida\n",
    "num_layers = 1       # Número de capas de la LSTM\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "\n",
    "X_train_batch, y_train_batch = DL.create_batches(X_train, y_train, batch_size)\n",
    "lstm_ = LSTM2.LSTMClassifier(input_size, hidden_size, num_classes, num_layers)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_.parameters(), lr=learning_rate)\n",
    "lstm_.train_model(X_train_batch, y_train_batch, criterion, optimizer, num_epochs=20)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"x, y, actors = AP.get_dataset([speech8_dataset])\n",
    "X_train, X_test, y_train, y_test, actors_train, actors_test = AP.split_dataset(x, y, test_size=0.2, actors=actors)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(actors_train.shape)\n",
    "print(actors_test.shape)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid, actors_train, actors_valid = AP.split_dataset(X_train, y_train, test_size=0.2, actors=actors_train)\n",
    "print(\"train:\", X_train.shape)\n",
    "print(\"valid:\", X_valid.shape)\n",
    "\n",
    "plt.hist(y_train)\n",
    "plt.show()\n",
    "\n",
    "y_train = one_hot_encoder(y_train)\n",
    "y_valid = one_hot_encoder(y_valid)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "shape_train = X_train.shape\n",
    "X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "X_train = X_train_scaled.reshape(shape_train[0], shape_train[1], shape_train[2])\n",
    "\n",
    "shape_valid = X_valid.shape\n",
    "X_val_reshaped = X_valid.reshape(-1, X_valid.shape[-1])\n",
    "X_val_scaled = scaler.transform(X_val_reshaped)\n",
    "X_val = X_val_scaled.reshape(shape_valid[0], shape_valid[1], shape_valid[2])\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_valid, y_valid))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"y_test = one_hot_encoder(y_test)\n",
    "\n",
    "shape = X_test.shape\n",
    "X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
    "X_test_scaled = scaler.transform(X_test_reshaped)\n",
    "X_test = X_test_scaled.reshape(shape[0], shape[1], shape[2])\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convertir las predicciones de one-hot encoding a etiquetas\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(y_pred_labels)\n",
    "print(y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO CORRER, USAR Dataloader.get_dataset() y Dataloader.split_dataset()\n",
    "DL.process_dataset(speech_unprocessed_path, speech_dataset_path)\n",
    "DL.process_dataset(song_unprocessed_path, song_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A partir de aca el TEST no se toca mas.\n",
    "# Hay que hacer otra division del dev set para formar el train set y valid set.\n",
    "\n",
    "x_speech, y_speech, actors_speech = DL.get_dataset([speech_dataset_path])\n",
    "x_song, y_song, actors_song = DL.get_dataset([song_dataset_path])\n",
    "x_both, y_both, actors_both = DL.get_dataset([speech_dataset_path, song_dataset_path])\n",
    "\n",
    "# Estos son los datasets DIVIDIDOS POR ACTORES para el dev set y test set.\n",
    "X_speech_byactors_dev, X_speech_byactors_test, y_speech_byactors_dev, y_speech_byactors_test, actors_speech_dev, actors_speech_test = DL.split_dataset(x_speech, y_speech, actors_speech)\n",
    "X_song_byactors_dev, X_song_byactors_test, y_song_byactors_dev, y_song_byactors_test, actors_song_dev, actors_song_test = DL.split_dataset(x_song, y_song, actors_song)\n",
    "X_both_byactors_dev, X_both_byactors_test, y_both_byactors_dev, y_both_byactors_test, actors_both_dev, actors_both_test = DL.split_dataset(x_both, y_both, actors_both)\n",
    "\n",
    "# Estos son los datasets NO divididos por actores para el dev set y test set.\n",
    "X_speech_dev, X_speech_test, y_speech_dev, y_speech_test, _ , _ = DL.split_dataset(x_speech, y_speech)\n",
    "X_song_dev, X_song_test, y_song_dev, y_song_test, _ , _ = DL.split_dataset(x_song, y_song)\n",
    "X_both_dev, X_both_test, y_both_dev, y_both_test, _ , _ = DL.split_dataset(x_both, y_both)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIVISION ENTRE TRAIN SET Y VALIDATION SET\n",
    "\n",
    "X_speech_byactors_train, X_speech_byactors_valid, y_speech_byactors_train, y_speech_byactors_valid, actors_speech_train, actors_speech_valid = DL.split_dataset(X_speech_byactors_dev, y_speech_byactors_dev, actors_speech_dev)\n",
    "X_song_byactors_train, X_song_byactors_valid, y_song_byactors_train, y_song_byactors_valid, actors_song_train, actors_song_valid = DL.split_dataset(X_song_byactors_dev, y_song_byactors_dev, actors_song_dev)\n",
    "X_both_byactors_train, X_both_byactors_valid, y_both_byactors_train, y_both_byactors_valid, actors_both_train, actors_both_valid = DL.split_dataset(X_both_byactors_dev, y_both_byactors_dev, actors_both_dev)\n",
    "\n",
    "X_speech_train, X_speech_valid, y_speech_train, y_speech_valid, _ , _ = DL.split_dataset(X_speech_dev, y_speech_dev)\n",
    "X_song_train, X_song_valid, y_song_train, y_song_valid, _ , _ = DL.split_dataset(X_song_dev, y_song_dev)\n",
    "X_both_train, X_both_valid, y_both_train, y_both_valid, _ , _ = DL.split_dataset(X_both_dev, y_both_dev)\n",
    "\n",
    "\n",
    "### habria que hacer una funcion que devuelva especificamente el train, valid del dataset que queremos. Ahora lo hice asi para ya tenerlos todos creados y poder probarlos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_norm = normalization(X_speech_byactors_train, np.mean(X_speech_byactors_train, axis=0), np.std(X_speech_byactors_train, axis=0))\n",
    "x_valid_norm = normalization(X_speech_byactors_valid, np.mean(X_speech_byactors_train, axis=0), np.std(X_speech_byactors_train, axis=0))\n",
    "print(x_train_norm.shape)\n",
    "print(x_valid_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_speech, classes_count_speech = np.unique(y_speech, return_counts=True)\n",
    "classes_song, classes_count_song = np.unique(y_song, return_counts=True)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4)) \n",
    "\n",
    "axs[0].bar(classes_speech, classes_count_speech)\n",
    "axs[0].set_title('Speech dataset')\n",
    "axs[0].set_xlabel('Clase')\n",
    "axs[0].set_ylabel('Cantidad de muestras')\n",
    "axs[0].set_xticks(classes_speech) \n",
    "\n",
    "axs[1].bar(classes_song, classes_count_song)\n",
    "axs[1].set_title('Song dataset')\n",
    "axs[1].set_xlabel('Clase')\n",
    "axs[1].set_ylabel('Cantidad de muestras')\n",
    "axs[1].set_xticks(classes_song)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#NOTE: There is no strong intensity for the 'neutral' emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(y_speech))\n",
    "print(np.unique(y_song))\n",
    "\n",
    "# y_speech = y_speech.astype(int)\n",
    "# y_song = y_song.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_speech = StandardScaler()\n",
    "x_speech_scaled = scaler_speech.fit_transform(x_speech)\n",
    "\n",
    "scaler_song = StandardScaler()\n",
    "x_song_scaled = scaler_song.fit_transform(x_song)\n",
    "\n",
    "pca_speech = PCA(n_components=2)\n",
    "pca_speech_result = pca_speech.fit_transform(x_speech_scaled)\n",
    "pca_speech_df = pd.DataFrame(data=pca_speech_result, columns=['PCA1', 'PCA2'])\n",
    "pca_speech_df['Label'] = y_speech\n",
    "\n",
    "pca_song = PCA(n_components=2)\n",
    "pca_song_result = pca_song.fit_transform(x_song_scaled)\n",
    "pca_song_df = pd.DataFrame(data=pca_song_result, columns=['PCA1', 'PCA2'])\n",
    "pca_song_df['Label'] = y_song\n",
    "\n",
    "sns.scatterplot(x='PCA1', y='PCA2', hue='Label', data=pca_speech_df, palette='viridis')\n",
    "plt.title('PCA of Speech Features')\n",
    "plt.legend(title='Emotion')\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='PCA1', y='PCA2', hue='Label', data=pca_song_df, palette='viridis')\n",
    "plt.title('PCA of Song Features')\n",
    "plt.legend(title='Emotion')\n",
    "plt.show()\n",
    "\n",
    "tsne_speech = TSNE(n_components=2, random_state=42)\n",
    "tsne_speech_result = tsne_speech.fit_transform(x_speech_scaled)\n",
    "tsne_speech_df = pd.DataFrame(data=tsne_speech_result, columns=['TSNE1', 'TSNE2'])\n",
    "tsne_speech_df['Label'] = y_speech\n",
    "\n",
    "tsne_song = TSNE(n_components=2, random_state=42)\n",
    "tsne_song_result = tsne_song.fit_transform(x_song_scaled)\n",
    "tsne_song_df = pd.DataFrame(data=tsne_song_result, columns=['TSNE1', 'TSNE2'])\n",
    "tsne_song_df['Label'] = y_song\n",
    "\n",
    "sns.scatterplot(x='TSNE1', y='TSNE2', hue='Label', data=tsne_speech_df, palette='viridis')\n",
    "plt.title('t-SNE of Speech Features')\n",
    "plt.legend(title='Emotion')\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='TSNE1', y='TSNE2', hue='Label', data=tsne_song_df, palette='viridis')\n",
    "plt.title('t-SNE of Song Features')\n",
    "plt.legend(title='Emotion')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filename identifiers \n",
    "\n",
    "* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "* Vocal channel (01 = speech, 02 = song).\n",
    "* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_both_train, X_both_test, y_both_train, y_both_test\n",
    "\n",
    "num_trees = range(150, 200)\n",
    "max_depth = range(3, 9, 2)\n",
    "tree_model_accuracy = []\n",
    "opt_md = 0\n",
    "opt_n = 0\n",
    "for md in range(len(max_depth)):\n",
    "    for n in range(len(num_trees)):\n",
    "        random_forest = RandomForestClassifier(n_estimators=num_trees[n], max_depth=max_depth[md],random_state= 32)\n",
    "        random_forest.fit(X_speech_train, y_speech_train)\n",
    "        y_predict = random_forest.predict(X_speech_test)\n",
    "        acc = accuracy_score(y_speech_test, y_predict)\n",
    "        tree_model_accuracy.append(acc)\n",
    "        if np.max(tree_model_accuracy) == acc:\n",
    "            opt_md = md\n",
    "            opt_n = n\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"opt md: {max_depth[opt_md]}, opt n: {num_trees[opt_n]}\")\n",
    "print(f\"Max accuracy: {np.max(tree_model_accuracy)}\")\n",
    "plt.plot(range(len(tree_model_accuracy)), tree_model_accuracy)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier( random_state= 32)\n",
    "random_forest.fit(x_train, y_train)\n",
    "y_predict = random_forest.predict(x_test)\n",
    "acc = accuracy_score(y_test, y_predict)\n",
    "print(f\" trees --> Accuracy {acc}\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_predict)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPAutoencoder(nn.Module):\n",
    "    def __init__(self, encoding_dim):\n",
    "        super(MLPAutoencoder, self).__init__()\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.encoder = nn.Linear(88, encoding_dim)\n",
    "        self.decoder = nn.Linear(encoding_dim, 88)\n",
    "        #self.unflatten = torch.nn.Unflatten(1, 88)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        out = F.relu(self.encoder(x))\n",
    "        out = torch.sigmoid(self.decoder(out))\n",
    "        return out\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        return F.relu(self.encoder(x))\n",
    "d = torch.from_numpy(x_train)\n",
    "print(d.shape)\n",
    "print(nn.Flatten(d))\n",
    "print(d[0].shape)\n",
    "print(d)\n",
    "print(d.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPAutoencoder(33).double()\n",
    "pred = model(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.1\n",
    "device = \"cpu\"\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "n_epochs = 20\n",
    "model.to(device)\n",
    "t_losses, v_losses = [], []\n",
    "x_train_tensor = torch.from_numpy(x_train_norm)\n",
    "x_test_tensor = torch.from_numpy(x_test_norm)\n",
    "\n",
    "for epoch in tqdm(range(1, n_epochs+1)):\n",
    "    # Training\n",
    "    train_loss = 0.0\n",
    "    for (X, y) in zip(x_train_tensor, y_train):\n",
    "        X = torch.squeeze(X).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, X)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "            \n",
    "    # Eval        \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (X, y) in zip(x_test_tensor, y_test):\n",
    "            X = torch.squeeze(X).to(device)\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, X)\n",
    "            test_loss += loss.item()        \n",
    "            \n",
    "    train_loss = train_loss/len(x_train)\n",
    "    test_loss = test_loss/len(x_test)\n",
    "    t_losses.append(train_loss)\n",
    "    v_losses.append(test_loss)\n",
    "    \n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(np.arange(1,len(t_losses)+1), t_losses, label='Train loss')\n",
    "plt.plot(np.arange(1,len(v_losses)+1),v_losses, label='Validation loss')\n",
    "plt.xticks(np.arange(1,len(t_losses)+1))\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales\n",
    "\n",
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "emotion_labels = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "def extract_spectrogram(audio_path, sr=22050, n_mels=224, fmax=8000):\n",
    "    y, sr = librosa.load(audio_path, sr=sr)\n",
    "    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmax=fmax)\n",
    "    log_spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "    return log_spectrogram\n",
    "\n",
    "def save_spectrogram(spectrogram, save_path):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    librosa.display.specshow(spectrogram, sr=22050, x_axis='time', y_axis='mel', fmax=8000)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "def process_files(files, output_path):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    for audio_path in files:\n",
    "        filename = os.path.basename(audio_path)\n",
    "        emotion_code = filename.split('-')[2] \n",
    "        emotion = emotion_labels.get(emotion_code, 'unknown')\n",
    "        \n",
    "        class_dir = os.path.join(output_path, emotion)\n",
    "        if not os.path.exists(class_dir):\n",
    "            os.makedirs(class_dir)\n",
    "        \n",
    "        spectrogram = extract_spectrogram(audio_path)\n",
    "        save_path = os.path.join(class_dir, filename.replace('.wav', '.png'))\n",
    "        save_spectrogram(spectrogram, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 28 * 28, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 8)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 32 * 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "emotion_labels = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "def extract_spectrogram(audio_path, sr=22050, n_mels=224, fmax=8000):\n",
    "    y, sr = librosa.load(audio_path, sr=sr)\n",
    "    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmax=fmax)\n",
    "    log_spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "    return log_spectrogram\n",
    "\n",
    "def save_spectrogram(spectrogram, save_path):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    librosa.display.specshow(spectrogram, sr=22050, x_axis='time', y_axis='mel', fmax=8000)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "def process_files(files, output_path):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    for audio_path in files:\n",
    "        filename = os.path.basename(audio_path)\n",
    "        emotion_code = filename.split('-')[2] \n",
    "        emotion = emotion_labels.get(emotion_code, 'unknown')\n",
    "        \n",
    "        class_dir = os.path.join(output_path, emotion)\n",
    "        if not os.path.exists(class_dir):\n",
    "            os.makedirs(class_dir)\n",
    "        \n",
    "        spectrogram = extract_spectrogram(audio_path)\n",
    "        save_path = os.path.join(class_dir, filename.replace('.wav', '.png'))\n",
    "        save_spectrogram(spectrogram, save_path)\n",
    "\n",
    "dev_path = \"data/data_dev/speech/*/*.wav\"\n",
    "test_path = \"data/data_test/speech/*/*.wav\"\n",
    "\n",
    "files = glob(dev_path)\n",
    "output_path = \"data/spectrograms/speech\"\n",
    "process_files(files, output_path)\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "momentum = 0.8\n",
    "epochs = 20\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=output_path, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, optimizer, and loss function\n",
    "model = CNN()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 100 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")\n",
    "\n",
    "# Testing the model\n",
    "test_files = glob(test_path)\n",
    "test_output_path = \"data/spectrograms_test/speech\"\n",
    "process_files(test_files, test_output_path)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root=test_output_path, transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy of the network on the test images: {accuracy:.2f}%\")\n",
    "\n",
    "# Predicting probabilities for the 8 emotions\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        print(probabilities.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_path = \"data/data_dev/speech/*/*.wav\"\n",
    "test_path = \"data/data_test/speech/*/*.wav\"\n",
    "\n",
    "files = glob(dev_path)\n",
    "output_path = \"data/spectrograms/speech\"\n",
    "process_files(files, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10] loss: 2.133\n",
      "[1, 20] loss: 2.133\n",
      "[1, 30] loss: 2.137\n",
      "[2, 10] loss: 2.121\n",
      "[2, 20] loss: 2.140\n",
      "[2, 30] loss: 2.149\n",
      "[3, 10] loss: 2.130\n",
      "[3, 20] loss: 2.137\n",
      "[3, 30] loss: 2.137\n",
      "[4, 10] loss: 2.137\n",
      "[4, 20] loss: 2.149\n",
      "[4, 30] loss: 2.143\n",
      "[5, 10] loss: 2.130\n",
      "[5, 20] loss: 2.162\n",
      "[5, 30] loss: 2.118\n",
      "[6, 10] loss: 2.143\n",
      "[6, 20] loss: 2.105\n",
      "[6, 30] loss: 2.155\n",
      "[7, 10] loss: 2.149\n",
      "[7, 20] loss: 2.118\n",
      "[7, 30] loss: 2.140\n",
      "[8, 10] loss: 2.130\n",
      "[8, 20] loss: 2.127\n",
      "[8, 30] loss: 2.152\n",
      "[9, 10] loss: 2.143\n",
      "[9, 20] loss: 2.137\n",
      "[9, 30] loss: 2.146\n",
      "[10, 10] loss: 2.140\n",
      "[10, 20] loss: 2.152\n",
      "[10, 30] loss: 2.105\n",
      "[11, 10] loss: 2.115\n",
      "[11, 20] loss: 2.149\n",
      "[11, 30] loss: 2.130\n",
      "[12, 10] loss: 2.130\n",
      "[12, 20] loss: 2.165\n",
      "[12, 30] loss: 2.118\n",
      "[13, 10] loss: 2.137\n",
      "[13, 20] loss: 2.143\n",
      "[13, 30] loss: 2.140\n",
      "[14, 10] loss: 2.155\n",
      "[14, 20] loss: 2.124\n",
      "[14, 30] loss: 2.127\n",
      "[15, 10] loss: 2.121\n",
      "[15, 20] loss: 2.137\n",
      "[15, 30] loss: 2.133\n",
      "[16, 10] loss: 2.121\n",
      "[16, 20] loss: 2.112\n",
      "[16, 30] loss: 2.152\n",
      "[17, 10] loss: 2.149\n",
      "[17, 20] loss: 2.152\n",
      "[17, 30] loss: 2.130\n",
      "[18, 10] loss: 2.140\n",
      "[18, 20] loss: 2.112\n",
      "[18, 30] loss: 2.143\n",
      "[19, 10] loss: 2.130\n",
      "[19, 20] loss: 2.140\n",
      "[19, 30] loss: 2.137\n",
      "[20, 10] loss: 2.108\n",
      "[20, 20] loss: 2.152\n",
      "[20, 30] loss: 2.162\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 28 * 28, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 8)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 32 * 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "momentum = 0.8\n",
    "epochs = 20\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=output_path, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# model = CNN()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(dataloader, 0):\n",
    "#         inputs, labels = data\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         running_loss += loss.item()\n",
    "#         if i % 10 == 9:  \n",
    "#             print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 10:.3f}\")\n",
    "#             running_loss = 0.0\n",
    "\n",
    "model = CNN()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99: \n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'emotion_net.pth')\n",
    "\n",
    "# model.load_state_dict(torch.load('emotion_net.pth'))\n",
    "# model.eval()\n",
    "\n",
    "# test_dataset = datasets.ImageFolder(root=\"pathdeltest\", transform=transform)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     for data in test_dataloader:\n",
    "#         images, labels = data\n",
    "#         outputs = model(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "# print(f'Accuracy of the network on the test images: {100 * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN_X(Model):\n",
    "#     def __init__(self):\n",
    "#         super(CNN_X, self).__init__()\n",
    "#         self.conv1 = Conv2D(8, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 1))\n",
    "#         self.avgpool1 = AveragePooling2D(pool_size=(2, 2))\n",
    "#         self.conv2 = Conv2D(16, kernel_size=(3, 3), activation='relu')\n",
    "#         self.avgpool2 = AveragePooling2D(pool_size=(2, 2))\n",
    "#         self.conv3 = Conv2D(32, kernel_size=(3, 3), activation='relu')\n",
    "#         self.avgpool3 = AveragePooling2D(pool_size=(2, 2))\n",
    "#         self.flatten = Flatten()\n",
    "#         self.fc1 = Dense(2048, activation='relu')\n",
    "#         self.fc2 = Dense(2048, activation='relu')\n",
    "#         self.fc3 = Dense(8, activation='softmax')\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x = self.conv1(inputs)\n",
    "#         x = self.avgpool1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.avgpool2(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.avgpool3(x)\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return self.fc3(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
